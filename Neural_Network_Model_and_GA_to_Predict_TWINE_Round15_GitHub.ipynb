{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedfadul/DeepSearch-Finding-Truncated-Differential-Trails-Using-Genetic-Algorithms-and-Deep-Learning/blob/main/Neural_Network_Model_and_GA_to_Predict_TWINE_Round15_GitHub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwSw8INOSUty"
      },
      "outputs": [],
      "source": [
        " # Use seaborn for pairplot\n",
        "!pip install -q seaborn\n",
        "# Use some functions from tensorflow_docs\n",
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "!pip install geneticalgorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoEVS6pnS8aP"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7H2-aq4DZLR"
      },
      "outputs": [],
      "source": [
        "import errno\n",
        "import os\n",
        "import signal\n",
        "import functools\n",
        "\n",
        "class TimeoutError(Exception):\n",
        "    pass\n",
        "\n",
        "def timeout(seconds=10, error_message=os.strerror(errno.ETIME)):\n",
        "    def decorator(func):\n",
        "        def _handle_timeout(signum, frame):\n",
        "            raise TimeoutError(error_message)\n",
        "\n",
        "        @functools.wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            signal.signal(signal.SIGALRM, _handle_timeout)\n",
        "            signal.alarm(seconds)\n",
        "            try:\n",
        "                result = func(*args, **kwargs)\n",
        "            finally:\n",
        "                signal.alarm(0)\n",
        "            return result\n",
        "\n",
        "        return wrapper\n",
        "\n",
        "    return decorator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuFNpYuLTG7T",
        "outputId": "12203e29-3917-4d15-b8c5-ed0663003bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.11.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnNIsFK0TNdv"
      },
      "outputs": [],
      "source": [
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.plots\n",
        "import tensorflow_docs.modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmBs9mo9Tc4S"
      },
      "outputs": [],
      "source": [
        "def R_squared(y, y_pred):\n",
        "  y_pred = ops.convert_to_tensor_v2(y_pred)\n",
        "  y = math_ops.cast(y, y_pred.dtype)  \n",
        "  residual = tf.reduce_sum(tf.square(tf.subtract(y,y_pred)))\n",
        "  total = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n",
        "  r2 = tf.subtract(1.0, tf.math.divide(residual, total))\n",
        "  return r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFPK7m0hUZp9"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        y_pred = ops.convert_to_tensor_v2(y_pred)\n",
        "        y_true = math_ops.cast(y_true, y_pred.dtype)\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR8SbnrhkNHv"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dropout\n",
        "from keras import regularizers\n",
        "initializer = tf.keras.initializers.Constant(.0001)\n",
        "def build_model():\n",
        "  model = keras.Sequential([\n",
        "    layers.Dense(512, activation=tf.keras.activations.relu,\n",
        "                 input_shape=(49,),kernel_regularizer=regularizers.l2(0.1), kernel_initializer=initializer),\n",
        "    layers.Dropout(0.2),             \n",
        "    layers.Dense(512, activation=tf.keras.activations.relu,kernel_regularizer=regularizers.l2(0.1), kernel_initializer=initializer),\n",
        "    layers.Dropout(0.2),             \n",
        "    layers.Dense(512, activation=tf.keras.activations.relu,kernel_regularizer=regularizers.l2(0.1), kernel_initializer=initializer),\n",
        "    layers.Dropout(0.2),             \n",
        "    layers.Dense(512, activation=tf.keras.activations.relu,kernel_regularizer=regularizers.l2(0.1), kernel_initializer=initializer),\n",
        "    layers.Dropout(0.2),              \n",
        "    layers.Dense(512, activation=tf.keras.activations.relu,kernel_regularizer=regularizers.l2(0.1), kernel_initializer=initializer),\n",
        "    layers.Dropout(0.2),                                \n",
        "    layers.Dense(1)\n",
        "  ])\n",
        "  optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001)\n",
        "  loss = tf.losses.huber\n",
        "  model.compile(loss=loss,\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mae',root_mean_squared_error,R_squared])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJsn5rGekQBW"
      },
      "outputs": [],
      "source": [
        "model = build_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMQAx0BfLv8T"
      },
      "outputs": [],
      "source": [
        "model.load_weights('/content/drive/My Drive/Colab Notebooks/Exp6 Data/Exp6 BaseLine mse.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zVLAfz0Wwpc"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dropout\n",
        "from keras import regularizers\n",
        "from keras.regularizers import l2, l1\n",
        "reg_param=0.001\n",
        "\n",
        "def create_model():\n",
        "  model = keras.Sequential([\n",
        "        layers.Dense(512, activation='relu' , input_shape=[50]),\n",
        "        layers.Dense(512, activation='relu'),    \n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),     \n",
        "        layers.Dense(1,activation='sigmoid' )\n",
        "  ])\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001)\n",
        "  #mape = tf.keras.losses.MeanAbsolutePercentageError(tf.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
        "  loss = tf.losses.mape\n",
        "  model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.optimizers.Adam(learning_rate=0.001), metrics=(['accuracy'], ['Precision'],['Recall']))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGLtIQ40ugzh"
      },
      "outputs": [],
      "source": [
        "validity_model = create_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUQ79IlCvIDA"
      },
      "outputs": [],
      "source": [
        "validity_model.load_weights('/content/drive/My Drive/Colab Notebooks/Truncated path validity/truncated_validity_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9gWTn0SfBiu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from geneticalgorithm import geneticalgorithm as ga\n",
        "import random\n",
        "col=['input1', 'input2', 'input3', 'input4', 'input5', 'input6', 'input7', 'input8', 'input9', 'input10', 'input11', 'input12', 'input13', 'input14', 'input15', 'input16', 'output1', 'output2', 'output3', 'output4', 'output5', 'output6', 'output7', 'output8', 'output9', 'output10', 'output11', 'output12', 'output13', 'output14', 'output15', 'output16', 'sboxes', 'round', 'perm1', 'perm2', 'perm3', 'perm4', 'perm5', 'perm6', 'perm7', 'perm8', 'perm9', 'perm10', 'perm11', 'perm12', 'perm13', 'perm14', 'perm15', 'perm16']\n",
        "valid_data = pd.DataFrame(index=None, columns=col)\n",
        "\n",
        "def f(x): \n",
        "    arr=x\n",
        "    names =columns=test_dataset.columns\n",
        "    predcited_data = pd.DataFrame([arr], index=names, columns=names)\n",
        "    predcited_data =tf.slice(predcited_data,[0,0],[1,49])\n",
        "\n",
        "    pen=0\n",
        "    if x[0]+x[1]+x[2]+x[3]+x[4]+x[5]+x[6]+x[7]+x[8]+x[9]+x[10]+x[11]+x[12]+x[13]+x[14]+x[15]==0:\n",
        "        pen=100\n",
        "    if x[16]+x[17]+x[18]+x[19]+x[20]+x[21]+x[22]+x[23]+x[24]+x[25]+x[26]+x[27]+x[28]+x[29]+x[30]+x[31]==0:\n",
        "        pen=100\n",
        "    \n",
        "    predictedScaledSboxes = model.predict(predcited_data, verbose=0)\n",
        "    predictedScaledSboxes = (predictedScaledSboxes/100)\n",
        "    if (predictedScaledSboxes > 0.33) and (predictedScaledSboxes < 0.416) and (pen==0)  :\n",
        "        round_no = 15\n",
        "        input_output_diff = np.array(predcited_data[0][0:32])\n",
        "        input_output_diff = np.reshape(input_output_diff, (-1,32))\n",
        "        perm = np.array(predcited_data[0][33:49])\n",
        "        perm = np.reshape(perm, (-1,16))\n",
        "        startpoint = np.empty([1,50])\n",
        "        startpoint = np.column_stack((input_output_diff, predictedScaledSboxes, round_no, perm ))\n",
        "        startPointsTensor = pd.DataFrame(startpoint, index=None, columns=col)\n",
        "        global valid_data \n",
        "        valid_data = valid_data.append(startPointsTensor, ignore_index=True)\n",
        "        rows, cols = valid_data.shape\n",
        "        if rows >100 :\n",
        "          valid_data['round'] = (valid_data['round'])/100\n",
        "          predictions = validity_model.predict(valid_data, verbose=0)\n",
        "          rounded = [int(np.round(p[0])) for p in predictions]\n",
        "          startPoints = pd.DataFrame(index=None, columns=col)\n",
        "          from random import randint\n",
        "          for idx, r in enumerate(rounded) :\n",
        "            if (r == 1) :\n",
        "              startPoints = startPoints.append(valid_data.loc[idx], ignore_index=True)\n",
        "          num_rows, num_cols = startPoints.shape\n",
        "          print(num_rows)\n",
        "          for k in range(num_rows):\n",
        "            n = randint(1, num_rows)\n",
        "            f = open('/content/drive/My Drive/Colab Notebooks/Exp6 Data/Results/twine'+ str(k)+'.yaml', \"w\")\n",
        "            sweight = round(valid_data.iloc[n][32]*8*15)\n",
        "            sweight = str(sweight)\n",
        "            endweight = round(valid_data.iloc[n][32]*8*15)+1\n",
        "            endweight = str(endweight)\n",
        "            f.write(\"cipher: twine\\n\")\n",
        "            f.write(\"wordsize: 64\\n\")\n",
        "            f.write(\"mode: 0\\n\")\n",
        "            f.write(\"endweight: \" + endweight + \"\\n\")\n",
        "            f.write(\"sweight: \" + sweight + \"\\n\")\n",
        "            f.write(\"rounds: 15\\n\")\n",
        "            f.write(\"fixedVariables:\\n\")\n",
        "            for i in range(16) :\n",
        "              if (valid_data.iloc[n][i] == 0) :\n",
        "                a= (i*4)+3\n",
        "                b = (i*4)\n",
        "                f.write(\"- X0[\"+ str(a) + \":\" + str(b) + \"]: \\\"0x0\\\"\\n\")\n",
        "              elif (valid_data.iloc[n][i] == 1) :\n",
        "                a= (i*4)+3\n",
        "                b = (i*4)\n",
        "                f.write(\"- NOT (X0[\"+ str(a) + \":\" + str(b) +\"]: \\\"0x0)\\\"\\n\")\n",
        "            for i in range(16,32) :\n",
        "              if (valid_data.iloc[n][i] == 0) :\n",
        "                c = ((i-16)*4)+3\n",
        "                d = ((i-16)*4)\n",
        "                f.write(\"- X15[\" + str(c) + \":\" + str(d) + \"]: \\\"0x0\\\"\\n\")\n",
        "              elif (valid_data.iloc[n][i] == 1) :\n",
        "                c = ((i-16)*4)+3\n",
        "                d = ((i-16)*4)\n",
        "                f.write(\"- NOT (X15[\"+ str(c) + \":\" + str(d) + \"]: \\\"0x0)\\\"\\n\")\n",
        "            f.close()\n",
        "            f = open('/content/drive/My Drive/Colab Notebooks/Exp6 Data/Results/twine samples commands.txt', \"a\")\n",
        "            f.write(\"sudo python3 cryptosmt.py --input twine\"+ str(k)+\".yaml >>Results/result\" + str(k) + \".txt;\\n\")\n",
        "          sys.exit(0)\n",
        "\n",
        "\n",
        "          \n",
        "    return model.predict(predcited_data, verbose=0).flatten() + pen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD0SIlxcDKkb"
      },
      "outputs": [],
      "source": [
        "algorithm_param = {'max_num_iteration': 100,\\\n",
        "                   'population_size':200,\\\n",
        "                   'mutation_probability':0.1,\\\n",
        "                   'elit_ratio': 0.01,\\\n",
        "                   'crossover_probability': 0.5,\\\n",
        "                   'parents_portion': 0.3,\\\n",
        "                   'crossover_type':'uniform',\\\n",
        "                   'max_iteration_without_improv':100}\n",
        "                   \n",
        "varbound=np.array([[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0,1],[0.15,0.15],[test_dataset.perm1.values[0],test_dataset.perm1.values[0]],[test_dataset.perm2.values[0],test_dataset.perm2.values[0]],[test_dataset.perm3.values[0],test_dataset.perm3.values[0]],[test_dataset.perm4.values[0],test_dataset.perm4.values[0]],[test_dataset.perm5.values[0],test_dataset.perm5.values[0]],[test_dataset.perm6.values[0],test_dataset.perm6.values[0]],[test_dataset.perm7.values[0],test_dataset.perm7.values[0]],[test_dataset.perm8.values[0],test_dataset.perm8.values[0]],[test_dataset.perm9.values[0],test_dataset.perm9.values[0]],[test_dataset.perm10.values[0],test_dataset.perm10.values[0]],[test_dataset.perm11.values[0],test_dataset.perm11.values[0]],[test_dataset.perm12.values[0],test_dataset.perm12.values[0]],[test_dataset.perm13.values[0],test_dataset.perm13.values[0]],[test_dataset.perm14.values[0],test_dataset.perm14.values[0]],[test_dataset.perm15.values[0],test_dataset.perm15.values[0]],[test_dataset.perm16.values[0],test_dataset.perm16.values[0]]])\n",
        "vartype=np.array([['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['int'],['real'],['real'],['real'],['real'],['real'],['real'],['real'],['real'],['real'],['real'],['real'],['real'],['real'],['real'],['real'],['real'],['real']])\n",
        "GAmodel=ga(function=f,dimension=49,variable_type_mixed=vartype,variable_boundaries=varbound)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Vg-aJmG6rKv"
      },
      "outputs": [],
      "source": [
        "GAmodel.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1TMhOCkDO_K1VNh2a5y0ZJU0zOneKovem",
      "authorship_tag": "ABX9TyPYi3e6r1JhK4f6HcoEaoNr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}